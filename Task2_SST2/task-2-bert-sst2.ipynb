{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8133059,"sourceType":"datasetVersion","datasetId":4807314}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":3102.798711,"end_time":"2024-04-15T19:28:07.632117","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-15T18:36:24.833406","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"07f63ac385e245e69643261fa301d1e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a779e03f4ef4dfb9dc2d08b9d97ffa6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10ec51a0d39644c2bde1c044f4b0ebcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10fb04b92e174f849b69c56d203668ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a573658ed14c48d3a6a866cae319ac6c","IPY_MODEL_475ddaa2422245abb188229f13289e3b","IPY_MODEL_79acbae3e21e4376b30795092f52e8f2"],"layout":"IPY_MODEL_a13478a623724f66b1a14660d3ee9b4c"}},"128ec8e6c5d44978a2e9f175388e2f29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1cb8161b6d734b6a8a15af28275ef483":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f84540677f046efa31d6d90bb798393":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21e35c6ec57b4193bf50aecf1f702ac0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"252f4288a4c84d728c963178e69ad1f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74ed9df4556f4ce7a213050c48d7c210","IPY_MODEL_bdb76a9b14d2447d942fd2620266531f","IPY_MODEL_42fd17a164ad42feb46d98ca2d4e9c71"],"layout":"IPY_MODEL_dc1cc80bd35b4d5dae4679b8825368f6"}},"2c93f0851b684e0494d3c01044488537":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3683cb285f2b4634ae9fd869432cc86c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"387cac20b6294c009cf4f1f1d9440597":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed790f27239244649065667c37f56eb8","placeholder":"​","style":"IPY_MODEL_3c973380efdc46cd8415b5809cfa7f77","value":"model.safetensors: 100%"}},"3c973380efdc46cd8415b5809cfa7f77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42fd17a164ad42feb46d98ca2d4e9c71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_740886dea1fa48a9a06c17dadf3b6fbd","placeholder":"​","style":"IPY_MODEL_c63272069adc4b46a5bcc989ea18c0e0","value":" 232k/232k [00:00&lt;00:00, 5.70MB/s]"}},"475ddaa2422245abb188229f13289e3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b6601cf17dd46be9c10f6e87a1144aa","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_128ec8e6c5d44978a2e9f175388e2f29","value":466062}},"48b48c0a4e764642a4baa3380c8a9a8e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ab51461176848458df8f7aa1c18a9e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21e35c6ec57b4193bf50aecf1f702ac0","placeholder":"​","style":"IPY_MODEL_3683cb285f2b4634ae9fd869432cc86c","value":" 48.0/48.0 [00:00&lt;00:00, 3.54kB/s]"}},"5519dd8cd22445109116713dc5314117":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55bbfc2baca942e7837c0d480dd8d6de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58395f0c6881462ab960c9756ad249c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5eb0fe60340413fb5161245a1a5096f","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5519dd8cd22445109116713dc5314117","value":48}},"5b6601cf17dd46be9c10f6e87a1144aa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70e3c797bc7a4829b0c8b2bec78b05bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"740886dea1fa48a9a06c17dadf3b6fbd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74ed9df4556f4ce7a213050c48d7c210":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7db671e831e74462a2edde8154c087b8","placeholder":"​","style":"IPY_MODEL_0a779e03f4ef4dfb9dc2d08b9d97ffa6","value":"vocab.txt: 100%"}},"79acbae3e21e4376b30795092f52e8f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c66320e4daa1422fb28121fd94b5668a","placeholder":"​","style":"IPY_MODEL_1f84540677f046efa31d6d90bb798393","value":" 466k/466k [00:00&lt;00:00, 22.4MB/s]"}},"7db671e831e74462a2edde8154c087b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b67ebaa10884aff89c4aafe648550d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb019431ea60438095147927a0935bbb","placeholder":"​","style":"IPY_MODEL_b7235bc7b1ce45658a992e230c300b7a","value":"tokenizer_config.json: 100%"}},"8f4c677ab169495581975dd1851e0edf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cb8161b6d734b6a8a15af28275ef483","placeholder":"​","style":"IPY_MODEL_70e3c797bc7a4829b0c8b2bec78b05bd","value":" 570/570 [00:00&lt;00:00, 47.9kB/s]"}},"8f4e34afb9cf4030977b9940bfa279e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db400aad92ad449eb08c29ad43c38774","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c814b40c830c4f61ba82b1da0edd6c05","value":440449768}},"98bce00eebcd4bcdba25e51e41dfdb15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9db693f375a0422f9d84b6e2fb9b63f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb031d76dbc14c88975b16a64c1cddea","placeholder":"​","style":"IPY_MODEL_2c93f0851b684e0494d3c01044488537","value":"config.json: 100%"}},"a13478a623724f66b1a14660d3ee9b4c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4c828db3926417fa793da999dd17166":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a573658ed14c48d3a6a866cae319ac6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07f63ac385e245e69643261fa301d1e5","placeholder":"​","style":"IPY_MODEL_10ec51a0d39644c2bde1c044f4b0ebcf","value":"tokenizer.json: 100%"}},"a8b9dc282be24106806e603524f0e957":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2cb094195dd4c88b55eab4e7b32d99b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b67ebaa10884aff89c4aafe648550d8","IPY_MODEL_58395f0c6881462ab960c9756ad249c2","IPY_MODEL_4ab51461176848458df8f7aa1c18a9e7"],"layout":"IPY_MODEL_dc391f34db0e43ae96f1bc5079279edd"}},"b5ad6af9c7fb46e4a33bc078155a1fea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5ff972a4a0d452286e70738ffdf5ab6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9db693f375a0422f9d84b6e2fb9b63f1","IPY_MODEL_c6894eaf5ded4f9ab773f2b108874f37","IPY_MODEL_8f4c677ab169495581975dd1851e0edf"],"layout":"IPY_MODEL_48b48c0a4e764642a4baa3380c8a9a8e"}},"b7235bc7b1ce45658a992e230c300b7a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdb76a9b14d2447d942fd2620266531f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6cac9b8ec2540d6a4acfead90020e91","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_98bce00eebcd4bcdba25e51e41dfdb15","value":231508}},"c63272069adc4b46a5bcc989ea18c0e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c66320e4daa1422fb28121fd94b5668a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6894eaf5ded4f9ab773f2b108874f37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5ad6af9c7fb46e4a33bc078155a1fea","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef619ee29eca42038cd174dac32da185","value":570}},"c814b40c830c4f61ba82b1da0edd6c05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb031d76dbc14c88975b16a64c1cddea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7be6f8f41ca4f36a0e5beb3e49a1a1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8b9dc282be24106806e603524f0e957","placeholder":"​","style":"IPY_MODEL_55bbfc2baca942e7837c0d480dd8d6de","value":" 440M/440M [00:02&lt;00:00, 201MB/s]"}},"db400aad92ad449eb08c29ad43c38774":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc1cc80bd35b4d5dae4679b8825368f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc391f34db0e43ae96f1bc5079279edd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5eb0fe60340413fb5161245a1a5096f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6cac9b8ec2540d6a4acfead90020e91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb019431ea60438095147927a0935bbb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed790f27239244649065667c37f56eb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef619ee29eca42038cd174dac32da185":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3591226420045f883cad3c57057c3ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_387cac20b6294c009cf4f1f1d9440597","IPY_MODEL_8f4e34afb9cf4030977b9940bfa279e0","IPY_MODEL_d7be6f8f41ca4f36a0e5beb3e49a1a1e"],"layout":"IPY_MODEL_a4c828db3926417fa793da999dd17166"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning BERT on Stanford Sentiment Treebank 2\n\n# Introduction\n\nIn this project, we would use a variety of BERT models (BERT, RoBERTa, ALBERT, DistilBERT, DeBERTa) with the HuggingFace PyTorch library to fine-tune a model to fit the SST2 dataset. \n\n### BERT (Bidirectional Encoder Representations from Transformers) \n\nOriginal model code [here](https://github.com/google-research/bert) for NLP tasks.\n\nBERT consists of 12 Transformer Encoding layers (or 24 for large BERT)\n\n\n# Stanford Sentiment Treebank 2 (SST2) Dataset\nSST2 is a dataset for binary sentiment classification. It consists of sentences extracted from movie reviews and human annotations of their sentiment. The task is to determine whether a given sentence has positive or negative sentiment.\n\n## Data fields\n* sentence - the review sentence whose sentiment should be determined\n* label - 0 for negative sentiment, 1 for positive sentiment","metadata":{"papermill":{"duration":0.013721,"end_time":"2024-04-15T18:36:27.638861","exception":false,"start_time":"2024-04-15T18:36:27.625140","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"To make use of GPU, this notebook should be run on Google Colab or Kaggle. It is highly inadvisable to run this notebook on a local machine as it would take a very long time to train the model. However you can run on local for testing purposes.","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"papermill":{"duration":0.034812,"end_time":"2024-04-15T18:36:27.686693","exception":false,"start_time":"2024-04-15T18:36:27.651881","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:18.674069Z","iopub.execute_input":"2024-04-16T18:40:18.674519Z","iopub.status.idle":"2024-04-16T18:40:18.699664Z","shell.execute_reply.started":"2024-04-16T18:40:18.674493Z","shell.execute_reply":"2024-04-16T18:40:18.698814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Importing all necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom tqdm import tqdm\nfrom torch.utils.data import TensorDataset","metadata":{"papermill":{"duration":1.594531,"end_time":"2024-04-15T18:36:29.294223","exception":false,"start_time":"2024-04-15T18:36:27.699692","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:18.701058Z","iopub.execute_input":"2024-04-16T18:40:18.701325Z","iopub.status.idle":"2024-04-16T18:40:23.716744Z","shell.execute_reply.started":"2024-04-16T18:40:18.701302Z","shell.execute_reply":"2024-04-16T18:40:23.715966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Due to memory and time limit issues, we cannot really use the whole dataset for fine tuning. As a result, we would use a subset of the dataset for training and validation.","metadata":{}},{"cell_type":"code","source":"num_train_plus_test = None # Later, train and test data ratio is 80:20\nnum_val = None \nnum_unknown_labels_test = None\n# If you want to use all data, set values to None","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:40:23.717846Z","iopub.execute_input":"2024-04-16T18:40:23.718250Z","iopub.status.idle":"2024-04-16T18:40:23.722605Z","shell.execute_reply.started":"2024-04-16T18:40:23.718225Z","shell.execute_reply":"2024-04-16T18:40:23.721391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_sst2 = pd.read_csv(\"/kaggle/input/snlp-project-sst2/train.tsv\", sep=\"\\t\")\n# dev_sst2 = pd.read_csv(\"/kaggle/input/snlp-project-sst2/dev.tsv\", sep=\"\\t\")\n\ntrain_sst2 = pd.read_csv(\"/kaggle/input/snlp-project-sst2/train.tsv\", sep=\"\\t\", nrows=num_train_plus_test)\ndev_sst2 = pd.read_csv(\"/kaggle/input/snlp-project-sst2/dev.tsv\", sep=\"\\t\", nrows=num_val)\n\ntest_unknown_label_sst2 = pd.read_csv(\"/kaggle/input/snlp-project-sst2/test.tsv\", sep=\"\\t\", nrows=num_unknown_labels_test)\nprint(len(test_unknown_label_sst2))\ntrain_sst2.head()","metadata":{"papermill":{"duration":1.979732,"end_time":"2024-04-15T18:36:31.288278","exception":false,"start_time":"2024-04-15T18:36:29.308546","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:23.725566Z","iopub.execute_input":"2024-04-16T18:40:23.725940Z","iopub.status.idle":"2024-04-16T18:40:23.887419Z","shell.execute_reply.started":"2024-04-16T18:40:23.725909Z","shell.execute_reply":"2024-04-16T18:40:23.886448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BertForSequenceClassification\n\nFor this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n\nWe’ll be using [BertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification). This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.","metadata":{}},{"cell_type":"markdown","source":"### Choosing the BERT variant and the Tokenizer\n\nTo feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary. The tokenization must be performed by the tokenizer included with BERT. This Tokenizer is build upon the WordPiece tokenizer. ","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"The current device is\", device)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:40:23.888545Z","iopub.execute_input":"2024-04-16T18:40:23.888810Z","iopub.status.idle":"2024-04-16T18:40:23.946874Z","shell.execute_reply.started":"2024-04-16T18:40:23.888788Z","shell.execute_reply":"2024-04-16T18:40:23.945791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLEASE CHOOSE THE BERT VARIANT MODEL\n\nbert_model_name = \"bert-base-uncased\"\n# bert_model_name = \"roberta-base\"\n# bert_model_name = \"distilbert-base-uncased\"\n# bert_model_name = \"albert-base-v2\"\n# bert_model_name = \"deberta-base\"\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nbert_temp_name = \"microsoft/deberta-base\" if bert_model_name == \"deberta-base\" else bert_model_name\n\ntokenizer = AutoTokenizer.from_pretrained(bert_temp_name)\nbert_classification_model = AutoModelForSequenceClassification.from_pretrained(\n    pretrained_model_name_or_path=bert_temp_name,\n    num_labels=2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions=False, # Whether the model returns attentions weights.\n    output_hidden_states=False, # Whether the model returns all hidden-states.\n)\n\nbert_classification_model = bert_classification_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:40:23.948303Z","iopub.execute_input":"2024-04-16T18:40:23.948680Z","iopub.status.idle":"2024-04-16T18:40:30.557406Z","shell.execute_reply.started":"2024-04-16T18:40:23.948647Z","shell.execute_reply":"2024-04-16T18:40:30.556346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizing\nTo feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary. ","metadata":{"papermill":{"duration":0.013,"end_time":"2024-04-15T18:36:31.395538","exception":false,"start_time":"2024-04-15T18:36:31.382538","status":"completed"},"tags":[]}},{"cell_type":"code","source":"example_sentence = train_sst2['sentence'][0]\n\nprint(f\"Original sentence:\\n {example_sentence}\")\n\nexample_tokens = tokenizer.tokenize(example_sentence)\n\nprint(f\"\\nTokenized by {bert_model_name}:\\n {example_tokens}\")\n\nexample_tokens_ID = tokenizer.convert_tokens_to_ids(example_tokens)\nprint(f\"\\nToken IDs by {bert_model_name}:\\n {example_tokens_ID}\")","metadata":{"papermill":{"duration":0.02437,"end_time":"2024-04-15T18:36:37.527952","exception":false,"start_time":"2024-04-15T18:36:37.503582","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:30.558939Z","iopub.execute_input":"2024-04-16T18:40:30.559687Z","iopub.status.idle":"2024-04-16T18:40:30.578192Z","shell.execute_reply.started":"2024-04-16T18:40:30.559659Z","shell.execute_reply":"2024-04-16T18:40:30.577424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can combine ``tokenize`` and ``convert_tokens_to_ids`` with the method ``encode``:","metadata":{"papermill":{"duration":0.014173,"end_time":"2024-04-15T18:36:37.556061","exception":false,"start_time":"2024-04-15T18:36:37.541888","status":"completed"},"tags":[]}},{"cell_type":"code","source":"example_sentence= train_sst2['sentence'][0]\n\nprint(f\"Original sentence:\\n {example_sentence}\")\n\nexample_tokens_ID = tokenizer.encode(example_sentence)\n\nprint(f\"\\nToken IDs by {bert_model_name}:\\n {example_tokens_ID}\")","metadata":{"papermill":{"duration":0.023133,"end_time":"2024-04-15T18:36:37.595157","exception":false,"start_time":"2024-04-15T18:36:37.572024","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:30.582471Z","iopub.execute_input":"2024-04-16T18:40:30.583661Z","iopub.status.idle":"2024-04-16T18:40:30.597831Z","shell.execute_reply.started":"2024-04-16T18:40:30.583624Z","shell.execute_reply":"2024-04-16T18:40:30.597044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see we got two extra tokens at the start and at the end compared to previous versions. Let's see what they are:","metadata":{"papermill":{"duration":0.014143,"end_time":"2024-04-15T18:36:37.623786","exception":false,"start_time":"2024-04-15T18:36:37.609643","status":"completed"},"tags":[]}},{"cell_type":"code","source":"tokenizer.decode(example_tokens_ID)","metadata":{"papermill":{"duration":9.724492,"end_time":"2024-04-15T18:36:47.362514","exception":false,"start_time":"2024-04-15T18:36:37.638022","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:30.599032Z","iopub.execute_input":"2024-04-16T18:40:30.599546Z","iopub.status.idle":"2024-04-16T18:40:39.740052Z","shell.execute_reply.started":"2024-04-16T18:40:30.599518Z","shell.execute_reply":"2024-04-16T18:40:39.739135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Special Tokens\n``[SEP]`` - At the end of every sentence, we need to append the special ``[SEP]`` token.\n\nThis token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (remeber BERT was trained intially to perdict question-answering task).\n\n``[CLS]`` - For classification tasks, we must prepend the special ``[CLS]`` token to the beginning of every sentence.\n\nThis token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output.\n\nSo now let's see how to encode the two questions together:","metadata":{"papermill":{"duration":0.022156,"end_time":"2024-04-15T18:36:47.406755","exception":false,"start_time":"2024-04-15T18:36:47.384599","status":"completed"},"tags":[]}},{"cell_type":"code","source":"example_sentence = train_sst2['sentence'][0]\n\nencoded_pair = tokenizer.encode(example_sentence)\ntokenizer.decode(encoded_pair)","metadata":{"papermill":{"duration":0.028661,"end_time":"2024-04-15T18:36:47.458928","exception":false,"start_time":"2024-04-15T18:36:47.430267","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:39.743395Z","iopub.execute_input":"2024-04-16T18:40:39.743685Z","iopub.status.idle":"2024-04-16T18:40:39.751346Z","shell.execute_reply.started":"2024-04-16T18:40:39.743661Z","shell.execute_reply":"2024-04-16T18:40:39.750366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentence Length\n\nThe sentences in our dataset obviously have varying lengths, so how does BERT handle this?\n\nBERT has two constraints:\n\n1. All sentences must be padded or truncated to a single, fixed length.\n2. The maximum sentence length is 512 tokens.\nPadding is done with a special ``[PAD]`` token, which is at index 0 in the BERT vocabulary.\n\nThe maximum length does impact training and evaluation speed.\n\nBefore we are ready to encode our text, though, we need to decide on a maximum sentence length for padding / truncating to.\n\nLet's find the maximum length:","metadata":{"papermill":{"duration":0.014418,"end_time":"2024-04-15T18:36:47.488328","exception":false,"start_time":"2024-04-15T18:36:47.473910","status":"completed"},"tags":[]}},{"cell_type":"code","source":"tqdm.pandas()\ntrain_sst2[\"sentence_length\"] = train_sst2[\"sentence\"].progress_apply(lambda question: \n                                                        len(tokenizer.tokenize(question)))\n\nMAX_LENGTH = train_sst2[\"sentence_length\"].max()\n\nprint(\"The maximum sentence length is \", MAX_LENGTH)","metadata":{"papermill":{"duration":2.103722,"end_time":"2024-04-15T18:36:49.606625","exception":false,"start_time":"2024-04-15T18:36:47.502903","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:39.752648Z","iopub.execute_input":"2024-04-16T18:40:39.752993Z","iopub.status.idle":"2024-04-16T18:40:45.833003Z","shell.execute_reply.started":"2024-04-16T18:40:39.752962Z","shell.execute_reply":"2024-04-16T18:40:45.832161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we construct the training, validation, testing dataset with known labels and testing dataset with unknown labels","metadata":{"papermill":{"duration":0.016526,"end_time":"2024-04-15T18:36:49.639951","exception":false,"start_time":"2024-04-15T18:36:49.623425","status":"completed"},"tags":[]}},{"cell_type":"code","source":"X_train, X_test_known_labels, y_train, y_test_known_labels = train_test_split(train_sst2[[\"sentence\"]], \n                                                    train_sst2[\"label\"], test_size=0.2, random_state=42)\nX_val = dev_sst2[[\"sentence\"]]\ny_val = dev_sst2[\"label\"]\n\nX_test_unknown_labels = test_unknown_label_sst2[[\"sentence\"]]","metadata":{"papermill":{"duration":0.032564,"end_time":"2024-04-15T18:36:49.689023","exception":false,"start_time":"2024-04-15T18:36:49.656459","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:45.834142Z","iopub.execute_input":"2024-04-16T18:40:45.834436Z","iopub.status.idle":"2024-04-16T18:40:45.857591Z","shell.execute_reply.started":"2024-04-16T18:40:45.834407Z","shell.execute_reply":"2024-04-16T18:40:45.856575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize dataset and create Dataloader\n\nNow we’re ready to perform the real tokenization.\n\nThe tokenizer.encode_plus function combines multiple steps for us:\n\n1. Split the sentence into tokens.\n2. Add the special ``[CLS]`` and ``[SEP]`` tokens.\n3. Map the tokens to their IDs.\n4. Pad or truncate all sentences to the same length.\n5. Create the attention masks which explicitly differentiate real tokens from ``[PAD]`` tokens.\n\nDocumentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus).","metadata":{"papermill":{"duration":0.016289,"end_time":"2024-04-15T18:36:49.721709","exception":false,"start_time":"2024-04-15T18:36:49.705420","status":"completed"},"tags":[]}},{"cell_type":"code","source":"MAX_LENGTH = 64\n\ntokenizer.encode_plus(X_train.iloc[0]['sentence'],  \n                        max_length=MAX_LENGTH,\n                        padding='max_length',\n                        return_attention_mask=True,\n                        return_tensors='pt',\n                        truncation=True)","metadata":{"papermill":{"duration":0.101843,"end_time":"2024-04-15T18:36:49.840093","exception":false,"start_time":"2024-04-15T18:36:49.738250","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:45.858862Z","iopub.execute_input":"2024-04-16T18:40:45.859159Z","iopub.status.idle":"2024-04-16T18:40:45.870365Z","shell.execute_reply.started":"2024-04-16T18:40:45.859134Z","shell.execute_reply":"2024-04-16T18:40:45.869205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So what we got:\n\n1. input_ids - Token ids padded with 0 at the end.\n2. token_type_ids - This array indicates bert what is the first sentence and what is the seconds. In case of classification of only one sentance this array is redundant.\n3. attention_mask - The “Attention Mask” is simply an array of 1s and 0s indicating which tokens are padding and which aren’t. This mask tells the “Self-Attention” mechanism in BERT not to incorporate these ``[PAD]`` tokens into its interpretation of the sentence.\n\nAll the array are in the size of ``max_length``.","metadata":{"papermill":{"duration":0.016895,"end_time":"2024-04-15T18:36:49.873851","exception":false,"start_time":"2024-04-15T18:36:49.856956","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def convert_to_dataset_torch(features, labels):\n    input_ids = []\n    attention_masks = []\n    token_type_ids = []\n    for _, row in tqdm(features.iterrows(), total=features.shape[0]):\n        encoded_dict = tokenizer.encode_plus(row[\"sentence\"],  \n                                             max_length=MAX_LENGTH,\n                                             padding='max_length', \n                                             return_attention_mask=True, \n                                             return_tensors='pt', \n                                             truncation=True)\n        # print(encoded_dict.keys())\n        \n        # Add the encoded sentences to the list.\n        input_ids.append(encoded_dict['input_ids'])\n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n        \n        if bert_model_name in [\"bert-base-uncased\", \"albert-base-v2\", \"deberta-base\"]:\n            token_type_ids.append(encoded_dict[\"token_type_ids\"])\n        \n    # Convert the lists into tensors.\n    if bert_model_name in [\"bert-base-uncased\", \"albert-base-v2\", \"deberta-base\"]:\n\n        input_ids = torch.cat(input_ids, dim=0)\n        token_type_ids = torch.cat(token_type_ids, dim=0)\n        attention_masks = torch.cat(attention_masks, dim=0)\n        labels = torch.tensor(labels.values)\n        \n        print(\"Shape of input_ids is\", input_ids.shape)\n        print(\"Shape of token_type_ids is\", token_type_ids.shape)\n        print(\"Shape of attention_masks is\", attention_masks.shape)\n        print(\"Shape of labels is\", labels.shape)\n        \n        return TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n    \n    elif bert_model_name in [\"roberta-base\", \"distilbert-base-uncased\"]:\n        # roberta does not have token_type_ids\n        input_ids = torch.cat(input_ids, dim=0)\n        attention_masks = torch.cat(attention_masks, dim=0)\n        labels = torch.tensor(labels.values)\n        \n        print(\"Shape of input_ids is\", input_ids.shape)\n        print(\"Shape of attention_masks is\", attention_masks.shape)\n        print(\"Shape of labels is\", labels.shape)\n        \n        return TensorDataset(input_ids, attention_masks, labels)\n    ","metadata":{"papermill":{"duration":0.028634,"end_time":"2024-04-15T18:36:49.919151","exception":false,"start_time":"2024-04-15T18:36:49.890517","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:45.872120Z","iopub.execute_input":"2024-04-16T18:40:45.872453Z","iopub.status.idle":"2024-04-16T18:40:45.884632Z","shell.execute_reply.started":"2024-04-16T18:40:45.872426Z","shell.execute_reply":"2024-04-16T18:40:45.883443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We’ll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory:","metadata":{"papermill":{"duration":0.016871,"end_time":"2024-04-15T18:36:49.953559","exception":false,"start_time":"2024-04-15T18:36:49.936688","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_tensor = convert_to_dataset_torch(X_train, y_train)","metadata":{"papermill":{"duration":8.287055,"end_time":"2024-04-15T18:36:58.257606","exception":false,"start_time":"2024-04-15T18:36:49.970551","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:40:45.885767Z","iopub.execute_input":"2024-04-16T18:40:45.886030Z","iopub.status.idle":"2024-04-16T18:41:06.059019Z","shell.execute_reply.started":"2024-04-16T18:40:45.886007Z","shell.execute_reply":"2024-04-16T18:41:06.058111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_tensor = convert_to_dataset_torch(X_val, y_val)","metadata":{"papermill":{"duration":2.021747,"end_time":"2024-04-15T18:37:00.303924","exception":false,"start_time":"2024-04-15T18:36:58.282177","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:06.060268Z","iopub.execute_input":"2024-04-16T18:41:06.060980Z","iopub.status.idle":"2024-04-16T18:41:06.454741Z","shell.execute_reply.started":"2024-04-16T18:41:06.060944Z","shell.execute_reply":"2024-04-16T18:41:06.453765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing\n\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# The DataLoader needs to know our batch size for training, so we specify it here.\n\nbatch_size = 5\n\ncore_number = multiprocessing.cpu_count()\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train_tensor,  # The training samples.\n            sampler = RandomSampler(train_tensor), # Select batches randomly\n            batch_size = batch_size, # Trains with this batch size.\n            num_workers = core_number\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nval_dataloader = DataLoader(\n            val_tensor, # The validation samples.\n            sampler = SequentialSampler(val_tensor), # Pull out batches sequentially.\n            batch_size = batch_size, # Evaluate with this batch size.\n            num_workers = core_number\n        )","metadata":{"papermill":{"duration":0.035939,"end_time":"2024-04-15T18:37:00.365433","exception":false,"start_time":"2024-04-15T18:37:00.329494","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:06.456131Z","iopub.execute_input":"2024-04-16T18:41:06.456461Z","iopub.status.idle":"2024-04-16T18:41:06.464041Z","shell.execute_reply.started":"2024-04-16T18:41:06.456434Z","shell.execute_reply":"2024-04-16T18:41:06.462845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop\nHelper function for formatting elapsed times as ``hh:mm:ss``:","metadata":{"papermill":{"duration":0.025836,"end_time":"2024-04-15T18:37:05.155030","exception":false,"start_time":"2024-04-15T18:37:05.129194","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"papermill":{"duration":0.033386,"end_time":"2024-04-15T18:37:05.215284","exception":false,"start_time":"2024-04-15T18:37:05.181898","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:06.465204Z","iopub.execute_input":"2024-04-16T18:41:06.465541Z","iopub.status.idle":"2024-04-16T18:41:06.505994Z","shell.execute_reply.started":"2024-04-16T18:41:06.465514Z","shell.execute_reply":"2024-04-16T18:41:06.504799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This function trains the batches from the train_dataloader","metadata":{"papermill":{"duration":0.025281,"end_time":"2024-04-15T18:37:05.266175","exception":false,"start_time":"2024-04-15T18:37:05.240894","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train_batch(epoch, bert_classification_model, train_dataloader, optimizer, scheduler, metric):\n    \n    total_train_loss = 0\n    total_train_accuracy = 0\n    \n    for batch in tqdm(train_dataloader, desc=f\"Training epoch: {epoch}\", unit=\"batch\"):\n        \n        if bert_model_name in [\"bert-base-uncased\", \"albert-base-v2\", \"deberta-base\"]:\n\n            # Unpack batch from dataloader.\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            token_type_ids = batch[2].to(device)\n            labels = batch[3].to(device)\n            \n            bert_classification_model.zero_grad()\n            \n            # Forward pass\n            outputs = bert_classification_model(input_ids, \n                                token_type_ids=token_type_ids, \n                                attention_mask=attention_masks, \n                                labels=labels)\n            \n        elif bert_model_name in [\"roberta-base\", \"distilbert-base-uncased\"]:\n            # Unpack batch from dataloader.\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            labels = batch[2].to(device)\n            \n            bert_classification_model.zero_grad()\n            \n            # Forward pass\n            outputs = bert_classification_model(input_ids, \n                                attention_mask=attention_masks, \n                                labels=labels)\n            \n        loss = outputs.loss\n        logits = outputs.logits\n\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(bert_classification_model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        y_pred = np.argmax(logits.cpu().detach().numpy(), axis=1).flatten()\n        \n        total_train_accuracy += metric(labels.cpu(), y_pred)\n\n    return total_train_loss, total_train_accuracy","metadata":{"papermill":{"duration":0.036185,"end_time":"2024-04-15T18:37:05.328085","exception":false,"start_time":"2024-04-15T18:37:05.291900","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:06.507682Z","iopub.execute_input":"2024-04-16T18:41:06.508071Z","iopub.status.idle":"2024-04-16T18:41:06.521516Z","shell.execute_reply.started":"2024-04-16T18:41:06.508027Z","shell.execute_reply":"2024-04-16T18:41:06.520417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This function validates the batches from the val_dataloader","metadata":{"papermill":{"duration":0.024768,"end_time":"2024-04-15T18:37:05.379318","exception":false,"start_time":"2024-04-15T18:37:05.354550","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def val_batch(epoch, bert_classification_model, val_dataloader, metric):\n    total_val_loss = 0\n    total_val_accuracy = 0\n    predicted_logits , predicted_labels = [], []\n    \n    for batch in tqdm(val_dataloader, desc=f\"Validating epoch: {epoch}\", unit=\"batch\"):\n        # Unpack batch from dataloader.\n        \n        if bert_model_name in [\"bert-base-uncased\", \"albert-base-v2\", \"deberta-base\"]:\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            token_type_ids = batch[2].to(device)\n            labels = batch[3].to(device)\n\n            with torch.no_grad():\n                # Forward pass, calculate logit predictions.\n                outputs = bert_classification_model(input_ids, \n                                    token_type_ids=token_type_ids, \n                                    attention_mask=attention_masks,\n                                    labels=labels)\n                loss = outputs.loss\n                logits = outputs.logits\n                \n        elif bert_model_name in [\"roberta-base\", \"distilbert-base-uncased\"]:\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            labels = batch[2].to(device)\n\n            with torch.no_grad():\n                # Forward pass, calculate logit predictions.\n                outputs = bert_classification_model(input_ids, \n                                    attention_mask=attention_masks,\n                                    labels=labels)\n                loss = outputs.loss\n                logits = outputs.logits\n            \n        total_val_loss += loss.item()\n        \n        y_pred = np.argmax(logits.cpu().detach().numpy(), axis=1).flatten()\n        \n        total_val_accuracy += metric(labels.cpu(), y_pred)\n        \n        predicted_logits.extend(logits.cpu().detach().numpy().tolist())\n        predicted_labels.extend(y_pred.tolist())\n    \n    return total_val_loss, total_val_accuracy, predicted_logits, predicted_labels","metadata":{"papermill":{"duration":0.039203,"end_time":"2024-04-15T18:37:05.445136","exception":false,"start_time":"2024-04-15T18:37:05.405933","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:06.523119Z","iopub.execute_input":"2024-04-16T18:41:06.523471Z","iopub.status.idle":"2024-04-16T18:41:06.537698Z","shell.execute_reply.started":"2024-04-16T18:41:06.523434Z","shell.execute_reply":"2024-04-16T18:41:06.535919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This function predicts the classes of the batches from the test_dataloader. Now the test_dataloader only contains X_test without y_test (which is None)","metadata":{}},{"cell_type":"code","source":"def predict_batch(bert_classification_model, predict_dataloader):\n    \n    predicted_logits , predicted_labels = [], []\n    \n    for batch in tqdm(predict_dataloader, desc=f\"Predicting\", unit=\"batch\"):\n        # Unpack batch from dataloader.\n        \n        if bert_model_name in [\"bert-base-uncased\", \"albert-base-v2\", \"deberta-base\"]:\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n            token_type_ids = batch[2].to(device)\n\n            with torch.no_grad():\n                # Forward pass, calculate logit predictions.\n                outputs = bert_classification_model(input_ids, \n                                    token_type_ids=token_type_ids, \n                                    attention_mask=attention_masks)\n                logits = outputs.logits\n                \n        elif bert_model_name in [\"roberta-base\", \"distilbert-base-uncased\"]:\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n\n            with torch.no_grad():\n                # Forward pass, calculate logit predictions.\n                outputs = bert_classification_model(input_ids, \n                                    attention_mask=attention_masks)\n                logits = outputs.logits\n            \n        y_pred = np.argmax(logits.cpu().detach().numpy(), axis=1).flatten()\n                \n        predicted_logits.extend(logits.cpu().detach().numpy().tolist())\n        predicted_labels.extend(y_pred.tolist())\n    \n    return predicted_logits, predicted_labels","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:41:06.539455Z","iopub.execute_input":"2024-04-16T18:41:06.539859Z","iopub.status.idle":"2024-04-16T18:41:06.551097Z","shell.execute_reply.started":"2024-04-16T18:41:06.539822Z","shell.execute_reply":"2024-04-16T18:41:06.550129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\n\ndef fine_tuning_pretrained_model(epochs, bert_classification_model, \n                                 train_dataloader, \n                                 val_dataloader, \n                                 optimizer, \n                                 scheduler,\n                                 metric\n                                 ):\n    \n    fine_tuning_statistics = []\n    \n    for epoch in range(1, epochs + 1):\n\n        #################\n        # TRAINING PART #\n        #################\n\n        t_begin_train = time.time()\n        \n        bert_classification_model.train()\n        \n        total_train_loss, total_train_accuracy = train_batch(epoch,\n                                       bert_classification_model,\n                                       train_dataloader,\n                                       optimizer,\n                                       scheduler,\n                                       metric\n                                       )\n        \n        # Calculate the average training loss for all batches.\n        average_train_loss = total_train_loss / len(train_dataloader)\n        print(f\"  Average training loss: {average_train_loss}\")\n\n        # Calculate the average training accuracy for all batches.\n        average_train_accuracy = total_train_accuracy / len(train_dataloader)\n        print(f\"  Average training accuracy: {average_train_accuracy}\")\n        \n        t_end_train = time.time()\n\n        training_time = format_time(t_end_train - t_begin_train)\n        print(f\"  Training time took: {training_time}\")\n        \n        ###################\n        # VALIDATING PART #\n        ###################\n\n        t_begin_val = time.time()\n        \n        bert_classification_model.eval()\n        \n        total_val_loss, total_val_accuracy,\\\n        predicted_logits, predicted_labels = val_batch(epoch, \n                                                        bert_classification_model,\n                                                        val_dataloader,\n                                                        metric)\n        \n        # Calculate the average validation loss for all batches.\n        average_val_loss = total_val_loss / len(val_dataloader)\n        print(f\"  Average validation loss: {average_val_loss}\")\n\n        # Calculate the average validation accuracy for all batches.\n        average_val_accuracy = total_val_accuracy / len(val_dataloader)\n        print(f\"  Average validation accuracy: {average_val_accuracy}\")\n    \n        t_end_val = time.time()\n\n        validation_time = format_time(t_end_val - t_begin_val)\n        print(f\"  Validation time took: {validation_time}\")\n    \n        # Record all statistics from this epoch.\n        fine_tuning_statistics.append(\n            {\n                'epoch': epoch,\n                'train_loss': average_train_loss,\n                'train_accuracy': average_train_accuracy,\n                'train_time': training_time,\n                'val_loss': average_val_loss,\n                'val_accuracy': average_val_accuracy,\n                'val_time': validation_time\n            }\n        )\n        \n\n    print(\"\")\n    print(\"Training complete!\")\n\n    return fine_tuning_statistics","metadata":{"papermill":{"duration":0.040053,"end_time":"2024-04-15T18:37:05.566326","exception":false,"start_time":"2024-04-15T18:37:05.526273","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:06.552535Z","iopub.execute_input":"2024-04-16T18:41:06.552897Z","iopub.status.idle":"2024-04-16T18:41:06.578206Z","shell.execute_reply.started":"2024-04-16T18:41:06.552860Z","shell.execute_reply":"2024-04-16T18:41:06.577024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer & Learning Rate Scheduler\nNow that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n\nFor the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)):\n\n* Batch size: 16, 32\n* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n* Number of epochs: 2, 3, 4\n\nI chose:\n* Batch size: 10 (due to memory limit of Kaggle)\n* Learning rate: 2e-5\n* Number of epochs: 4\n\nWe’re ready to kick off the training:","metadata":{"papermill":{"duration":0.027716,"end_time":"2024-04-15T18:37:05.620847","exception":false,"start_time":"2024-04-15T18:37:05.593131","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom sklearn.metrics import accuracy_score\n\nadamw_optimizer = AdamW(bert_classification_model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\n\nfrom transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs. The BERT authors recommend between 2 and 4. \nepochs = 4\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\n\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(adamw_optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)\n\n# Finally, we start to train the model\nfine_tuning_statistics = fine_tuning_pretrained_model(\n    epochs,\n    bert_classification_model,\n    train_dataloader,\n    val_dataloader,\n    adamw_optimizer,\n    scheduler,\n    accuracy_score)","metadata":{"papermill":{"duration":2970.860026,"end_time":"2024-04-15T19:26:36.507068","exception":false,"start_time":"2024-04-15T18:37:05.647042","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:06.579685Z","iopub.execute_input":"2024-04-16T18:41:06.580091Z","iopub.status.idle":"2024-04-16T18:41:09.918096Z","shell.execute_reply.started":"2024-04-16T18:41:06.580054Z","shell.execute_reply":"2024-04-16T18:41:09.915435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let’s view the summary of the training process. First we create a folder to save the results for the BERT variant","metadata":{"papermill":{"duration":0.388697,"end_time":"2024-04-15T19:26:37.242050","exception":false,"start_time":"2024-04-15T19:26:36.853353","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if not os.path.exists(f\"{bert_model_name}\"):\n    os.makedirs(f\"{bert_model_name}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:41:09.919167Z","iopub.status.idle":"2024-04-16T18:41:09.919570Z","shell.execute_reply.started":"2024-04-16T18:41:09.919357Z","shell.execute_reply":"2024-04-16T18:41:09.919372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_stats = pd.DataFrame(fine_tuning_statistics).set_index('epoch')\n# Saving the fine_tuning_statistics\ndf_stats.to_csv(f\"{bert_model_name}/fine_tuning_statistics.csv\")\ndf_stats","metadata":{"papermill":{"duration":0.360694,"end_time":"2024-04-15T19:26:37.941464","exception":false,"start_time":"2024-04-15T19:26:37.580770","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:09.921626Z","iopub.status.idle":"2024-04-16T18:41:09.922004Z","shell.execute_reply.started":"2024-04-16T18:41:09.921827Z","shell.execute_reply":"2024-04-16T18:41:09.921843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\n\n%matplotlib inline\n\npyplot.plot(df_stats['train_loss'], 'b-o', label=\"training\")\npyplot.plot(df_stats['val_loss'], 'g-o', label=\"validation\")\npyplot.title(f\"{bert_model_name}: training & validation loss\", fontsize=15)\npyplot.xlabel(\"Epoch\", fontsize=14)\npyplot.ylabel(\"Loss\", fontsize=14)\npyplot.legend(frameon=False, fontsize=14)\npyplot.xticks(df_stats.index.values.tolist(), fontsize=12)\npyplot.yticks(fontsize=12)\npyplot.savefig(f\"{bert_model_name}/train_val_loss_plot.png\")\npyplot.show()","metadata":{"papermill":{"duration":0.656377,"end_time":"2024-04-15T19:26:38.936759","exception":false,"start_time":"2024-04-15T19:26:38.280382","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:09.923499Z","iopub.status.idle":"2024-04-16T18:41:09.923878Z","shell.execute_reply.started":"2024-04-16T18:41:09.923701Z","shell.execute_reply":"2024-04-16T18:41:09.923717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\n\n%matplotlib inline\n\npyplot.plot(df_stats['train_accuracy'], 'b-o', label=\"training\")\npyplot.plot(df_stats['val_accuracy'], 'g-o', label=\"validation\")\npyplot.title(f\"{bert_model_name}: training & validation accuracy\", fontsize=15)\npyplot.xlabel(\"Epoch\", fontsize=14)\npyplot.ylabel(\"Loss\", fontsize=14)\npyplot.legend(frameon=False, fontsize=14)\npyplot.xticks(df_stats.index.values.tolist(), fontsize=12)\npyplot.yticks(fontsize=12)\npyplot.savefig(f\"{bert_model_name}/train_val_accuracy_plot.png\")\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:41:09.925411Z","iopub.status.idle":"2024-04-16T18:41:09.925769Z","shell.execute_reply.started":"2024-04-16T18:41:09.925592Z","shell.execute_reply":"2024-04-16T18:41:09.925612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance on test set and prediction","metadata":{"papermill":{"duration":0.384476,"end_time":"2024-04-15T19:26:39.668675","exception":false,"start_time":"2024-04-15T19:26:39.284199","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(type(y_test_known_labels))\ntest_tensor_known_labels = convert_to_dataset_torch(X_test_known_labels, y_test_known_labels)\ntest_dataloader = DataLoader(test_tensor_known_labels, \n                             sampler=SequentialSampler(test_tensor_known_labels), \n                             batch_size=batch_size)\n\n#print(len(X_test_unknown_labels))\n#features_tensor = torch.tensor(X_test_unknown_labels.to_numpy(), dtype=torch.float32)\n#print(features_tensor.shape)\n\n# Create dummy labels tensor\n\ndummy_labels_series = pd.Series([-1] * len(X_test_unknown_labels))\n# print(X_test_unknown_labels.shape)\ntest_tensor_unknown_labels = convert_to_dataset_torch(X_test_unknown_labels, dummy_labels_series)\npredict_dataloader = DataLoader(test_tensor_unknown_labels, \n                             sampler=SequentialSampler(test_tensor_unknown_labels), \n                             batch_size=batch_size)","metadata":{"papermill":{"duration":2.355805,"end_time":"2024-04-15T19:26:42.364086","exception":false,"start_time":"2024-04-15T19:26:40.008281","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:09.927287Z","iopub.status.idle":"2024-04-16T18:41:09.927666Z","shell.execute_reply.started":"2024-04-16T18:41:09.927479Z","shell.execute_reply":"2024-04-16T18:41:09.927493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate on the test set with known labels\nWith the test set prepared, we can apply our fine-tuned model to generate predictions on the test set with known labels","metadata":{"papermill":{"duration":0.362368,"end_time":"2024-04-15T19:26:43.095895","exception":false,"start_time":"2024-04-15T19:26:42.733527","status":"completed"},"tags":[]}},{"cell_type":"code","source":"bert_classification_model.eval()\n\ntotal_val_loss, total_val_accuracy,\\\npredicted_logits_val, predicted_labels_val = val_batch(None, bert_classification_model, \n                                               test_dataloader, accuracy_score)","metadata":{"papermill":{"duration":61.396734,"end_time":"2024-04-15T19:27:44.840416","exception":false,"start_time":"2024-04-15T19:26:43.443682","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:09.929342Z","iopub.status.idle":"2024-04-16T18:41:09.929730Z","shell.execute_reply.started":"2024-04-16T18:41:09.929552Z","shell.execute_reply":"2024-04-16T18:41:09.929568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making prediction on the test set with unknown labels\n\nWith the predict set prepared, we can apply our fine-tuned model to generate predictions on the test set with unknown labels","metadata":{}},{"cell_type":"code","source":"bert_classification_model.eval()\n\npredicted_logits_test, predicted_labels_test = predict_batch(bert_classification_model, \n                                                   predict_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:41:09.931028Z","iopub.status.idle":"2024-04-16T18:41:09.931392Z","shell.execute_reply.started":"2024-04-16T18:41:09.931203Z","shell.execute_reply":"2024-04-16T18:41:09.931218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We construct the testing result with corresponding truth labels and predicted labels by the fine-tuned BERT model. This testing data is part of the training data (train.tsv)","metadata":{}},{"cell_type":"code","source":"testing_results_known_labels = pd.DataFrame()\ntesting_results_known_labels[\"sentence\"] = X_test_known_labels[\"sentence\"]\ntesting_results_known_labels[\"true_label\"] = y_test_known_labels\ntesting_results_known_labels[\"predicted_label\"] = predicted_labels_val\ntesting_results_known_labels.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:41:09.932666Z","iopub.status.idle":"2024-04-16T18:41:09.933011Z","shell.execute_reply.started":"2024-04-16T18:41:09.932843Z","shell.execute_reply":"2024-04-16T18:41:09.932858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We construct the testing result with no known labels (the test.tsv file) with the predicted labels by the fine-tuned BERT model","metadata":{}},{"cell_type":"code","source":"testing_results_unknown_labels = pd.DataFrame()\ntesting_results_unknown_labels[\"sentence\"] = X_test_unknown_labels[\"sentence\"]\ntesting_results_unknown_labels[\"predicted_label\"] = predicted_labels_test\ntesting_results_unknown_labels.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:41:09.934368Z","iopub.status.idle":"2024-04-16T18:41:09.934717Z","shell.execute_reply.started":"2024-04-16T18:41:09.934554Z","shell.execute_reply":"2024-04-16T18:41:09.934569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving testing results","metadata":{}},{"cell_type":"code","source":"testing_results_known_labels.to_csv(f\"{bert_model_name}/testing_results_known_labels.csv\", index=False)\ntesting_results_unknown_labels.to_csv(f\"{bert_model_name}/testing_results_unknown_labels.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T18:41:09.935716Z","iopub.status.idle":"2024-04-16T18:41:09.936048Z","shell.execute_reply.started":"2024-04-16T18:41:09.935881Z","shell.execute_reply":"2024-04-16T18:41:09.935895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install data-science-utils","metadata":{"papermill":{"duration":14.174761,"end_time":"2024-04-15T19:28:00.100426","exception":false,"start_time":"2024-04-15T19:27:45.925665","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:09.937167Z","iopub.status.idle":"2024-04-16T18:41:09.937539Z","shell.execute_reply.started":"2024-04-16T18:41:09.937338Z","shell.execute_reply":"2024-04-16T18:41:09.937352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ds_utils.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(y_test_known_labels, predicted_labels_val, [1, 0])\npyplot.savefig(f\"{bert_model_name}/confusion_matrix.png\")\npyplot.show()","metadata":{"papermill":{"duration":1.267661,"end_time":"2024-04-15T19:28:01.759154","exception":false,"start_time":"2024-04-15T19:28:00.491493","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:09.939026Z","iopub.status.idle":"2024-04-16T18:41:09.939404Z","shell.execute_reply.started":"2024-04-16T18:41:09.939189Z","shell.execute_reply":"2024-04-16T18:41:09.939207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the fine-tuned model\nFirst let's save our model with the method ``save_pretrained``, then save our tokenizer with ``save_pretrained``.\n\n**Pay Attension**: tokenizer's ``save_pretrained`` need to get the path to save as a string.\n\nIn reality, it is not required to run this cell. Obtaining the results is already enough. The saved model would be extremely heavy, and there are already fine tuned BERT models on HuggingFace","metadata":{"papermill":{"duration":0.375146,"end_time":"2024-04-15T19:28:02.560249","exception":false,"start_time":"2024-04-15T19:28:02.185103","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from pathlib import Path\n\noutput_dir = Path(\"__file__\").parents[0].absolute().joinpath(f\"{bert_model_name}/model_save\")\noutput_dir.mkdir(exist_ok=True)\n\n# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n# They can then be reloaded using `from_pretrained()`\n\n# model_to_save = bert_classification_model.module if hasattr(bert_classification_model, 'module') else bert_classification_model  # Take care of distributed/parallel training\n# model_to_save.save_pretrained(output_dir)\n# tokenizer.save_pretrained(str(output_dir.absolute()))","metadata":{"papermill":{"duration":1.28376,"end_time":"2024-04-15T19:28:04.217720","exception":false,"start_time":"2024-04-15T19:28:02.933960","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-16T18:41:09.940658Z","iopub.status.idle":"2024-04-16T18:41:09.940995Z","shell.execute_reply.started":"2024-04-16T18:41:09.940828Z","shell.execute_reply":"2024-04-16T18:41:09.940842Z"},"trusted":true},"execution_count":null,"outputs":[]}]}